<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>Low-Resource Vision Challenges for Foundation Models</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="Low-Resource Vision Challenges for Foundation Models" />
	<meta property="og:description" content="Under Review" />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px"><b>Low-Resource Vision Challenges for Foundation Models</b></span>
		<p> </p>
		<table align=center width=800px>
			<table align=center width=800px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://xiaobai1217.github.io">Yunhua Zhang</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://hazeldoughty.github.io/">Hazel Doughty</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href="https://www.ceessnoek.info/">Cees G.M. Snoek</a></span>
						</center>
					</td>
				</tr>
			</table>
			<p> </p>
			<table align=center width=900px>
				<tr>
					<td align=center width=300px>
						<center>
							<span style="font-size:24px"><a href="https://ivi.fnwi.uva.nl/vislab/">VIS Lab</a>, University of Amsterdam</span>
						</center>
					</td>

				</tr>
			</table>
			<p> </p>
			<table align=center width=640px>
				<tr>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=200px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/xiaobai1217/Low-Resource-Vision'>[Code]</a></span><br>
						</center>
					</td>
					<td align=center width=220px>
						<center>
							<span style="font-size:24px"><a href=''>[Datasets]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<p> </p>



	<table align=center width=1100px>
  			  <tr>
 	              <td width=370px>
  					<center>
  	                	<img src = "./resources/1st-figure.png" height="300px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=1100px>
					<tr>
		              <td align=center width=1100px>
		              <span style="font-size:15px"><b>Figure 1. High-Resource vs Low-Resource Vision.</b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>



	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				Low-resource settings are well-established in natural language processing, where many languages lack sufficient data for machine learning at scale. However, low-resource problems are under-explored in computer vision. In this paper, we strive to address this gap and explore the challenges of low-resource image tasks with vision foundation models. Thus, we first collect a benchmark of genuinely low-resource image data, covering historic maps, circuit diagrams, and mechanical drawings. These low-resource settings all share the three challenges of data scarcity, fine-grained differences, and the distribution shift from natural images to the specialized domain of interest. While existing foundation models have shown impressive generalizability, we find they cannot transfer well to our low-resource tasks. To begin to tackle the challenges of low-resource vision, we introduce one simple baseline per challenge. Specifically, we propose to i) enlarge the data space by generative models, ii) adopt the best sub-kernels to encode local regions for fine-grained difference discovery and iii) learn attention for specialized domains. Experiments on the three low-resource data sources in our benchmark demonstrate our proposals already provide a better baseline than common transfer learning, data augmentation, and fine-grained methods. This highlights the unique characteristics and challenges of low-resource vision for foundation models that warrant further investigation. 
			</td>
		</tr>
	</table>
	<br>


	<center><h1>Tasks</h1></center>

	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/tasks.png" height="500px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Figure 2. Low-Resource Image Transfer Evaluation Benchmark.</b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our three benchmark tasks are: (a) classifying circuit diagrams with the correct function, (b) retrieving the modern satellite map given an old map of a city, and (c) retrieving the mechanical drawing corresponding to a 3D photo of a component and vice versa.

				</td>
			</tr>
		</center>
	</table>

		<p> </p>


	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/datasets.png" height="100px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Table 1. Benchmark Statistics</b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>


	<center><h1>Low-Resource Vision Challenges</h1></center>

	<table align=center width=850px>
		<center>
			<tr>
				<td>
					<b>Challenge I: Data Scarcity. </b>The data available for training models for low-resource scenarios is extremely limited. This is demonstrated through the small amount of data we were able to find online for each low-resource task (see Table 1).
					<b>Challenge II: Fine-Grained. </b>Data that is low-resourceis also highly specialized, meaning differences between images are incredibly subtle and attention to fine-grained details is necessary to solve the task. For example, the component symbols are key to a circuitâ€™s purpose, not its layout. Similarly, in mechanical drawings, the components may only vary in the number of holes.  <br>
					<b>Challenge III:Specialized Domain. </b>Not only is the available data severely limited, but it has a significantly different appearance and comes from an entirely different domain to the natural images commonly used in vision tasks. This means it is both difficult to bootstrap the training data for low-resource tasks with existing datasets and models that are successful on natural images cannot be easily applied to the specialized domains of low-resource images. 
				</td>
			</tr>
		</center>
	</table>




	<table align=center width=850px>
		<center><h1>Baselines for the Low-Resource Challenges</h1></center>
		<tr>
			<td>
				Our goal is to adapt foundation models, pre-trained on large-scale datasets, to low-resource tasks. To better handle adaptation in low-resource vision, we introduce one baseline for each challenge. 
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=850px>
		<center><h2>Baseline I: Generated Data for Data Scarcity</h2></center>
		<tr>
			<td>
				We augment images with generative models, obtaining images close to the input image where the label is preserved as well as more diverse images which break the label. We use label-preserving images in the task loss and augment the label-breaking images for use in a contrastive loss.
			</td>
		</tr>
	</table>
	<br>





	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/baselineI.png" height="200px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Figure 3. Generated Data for Data Scarcity. </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>


	<table align=center width=850px>
		<center><h2>Baseline II: Tokenization for Fine-Grained</h2></center>
		<tr>
			<td>
				We divide the original linear projection of a pre-trained foundation model into sub-kernels. These sub-kernels can be applied to smaller areas of the image patch to attend to fine-grained details. We learn a weighting to combine the resulting features into patch-level features.
			</td>
		</tr>
	</table>
	<br>





	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/baselineII.png" height="200px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Figure 4. Tokenization for Fine-Grained. </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>



	<table align=center width=850px>
		<center><h2>Baseline III: Attention for Specialized Domains</h2></center>
		<tr>
			<td>
				We learn a set of global attention maps with common attention patterns particular to the specialized domain such as vertical and horizontal directions for circuit diagrams. For each token, we crop the corresponding region from the global attention map according to the location.
			</td>
		</tr>
	</table>
	<br>





	 <table align=center width=1100px>
  			  <tr>
 	              <td width=700px>
  					<center>
  	                	<img src = "./resources/baselineIII.png" height="200px"></img>
  	                	<br>
					</center>
  	              </td> 
		  		  <table align=center width=800px>
					<tr>
		              <td align=center width=600px>
		              <span style="font-size:15px"><b>Figure 5. Attention for Specialized Domains. </b></span>
					  </td>
					</tr>
				  </table>
				<br>
                </tr>
  		  </table>






	<br>
	<hr>
	<table align=center width=800px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href=""><img class="layered-paper-big" style="height:175px" src="./resources/paper.png"/></a></td>
			<td><span style="font-size:14pt">Yunhua Zhang, Hazel Doughty, Cees G.M. Snoek<br>
				<b>Low-Resource Vision Challenges for Foundation Models.<br>
				(hosted on <a href="">ArXiv</a>)<br>
				<!-- (<a href="./resources/camera-ready.pdf">camera ready</a>)<br> -->
				<span style="font-size:14pt"><a href="./resources/bibtex.txt">[Bibtex]<br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<table align=center width=600px>
							<td align=center width=400px>
						<center>
							<span style="font-size:24px">Contact</span>
						</center>
					</td>

						<td align=center width=400px>
						<center>
							<span style="font-size:24px"><a href='mailto: y.zhang9@uva.nl'>[Email]</a></span>
						</center>
					</td>

					<td align=center width=400px>
						<center>
							<span style="font-size:24px"><a href='https://twitter.com/yunhua_zhang'>[Twitter]</a></span>
						</center>
					</td>
	</table>

	<hr>
	<br>

	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This work is financially supported by the Inception Institute of Artificial Intelligence, the University of Amsterdam and the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Ministry of Economic Affairs and Climate Policy. <br>
					<br>
					This website template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

